<!-- saved from url=(0044)http://featureselection.asu.edu/tutorial.php -->
<html lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
	<meta name="description" content="">
	<meta name="author" content="">
	<link rel="shortcut icon" href="http://featureselection.asu.edu/images/favicon.ico">

	<title>Tutorial | Feature Selection @ ASU</title>

	<!-- Bootstrap core CSS -->
	<link href="./Tutorial _ Feature Selection @ ASU_files/bootstrap.min.css" rel="stylesheet">

	<!-- Custom styles for this template -->
	<link href="./Tutorial _ Feature Selection @ ASU_files/algorithms_style.css" rel="stylesheet">

	<!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
	<!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
	<script src="./Tutorial _ Feature Selection @ ASU_files/ie-emulation-modes-warning.js.download"></script>

	<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
	<!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body data-gr-c-s-loaded="true">
	<!-- NAVBAR
================================================== -->
	<div class="navbar-wrapper">
		<div class="container">

			<nav class="navbar navbar-inverse navbar-static-top">
				<div class="container">
					<div class="navbar-header">
						<button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
							data-target="#navbar" aria-expanded="false" aria-controls="navbar">
							<span class="sr-only">Toggle navigation</span>
							<span class="icon-bar"></span>
							<span class="icon-bar"></span>
							<span class="icon-bar"></span>
						</button>
						<a class="navbar-brand" href="tutorial.html#">FEATURE
							SELECTION</a>
					</div>
					<div id="navbar" class="navbar-collapse collapse">
						<ul class="nav navbar-nav">
							<li class="active"><a href="home.html">HOME</a></li>
							<li><a href="datasets.html">DATASETS</a></li>
							<li><a href="algorithms.html">ALGORITHMS</a></li>
							<!--<li><a href="donate_a_dataset.php">DONATE A DATASET</a></li>
	            <li><a href="donate_an_algorithm.php">DONATE AN ALGORITHM</a></li>-->
						</ul>
					</div>
				</div>
			</nav>

		</div>
	</div>

	<!-- end #NAVBAR -->


	<div class="container">
		<div class="jumbotron">
			<h2>An brief introduction on how to perform feature selection with scikit-feature</h2>
		</div>
		<div class="entry">
			<h2 class="text-info"><b>Loading an dataset</b></h2>
			<p>
				<font size="3" font="" style="line-height:2;">First, we start a Python interpreter from shell and then
					load the <a href="http://featureselection.asu.edu/files/datasets/COIL20.mat">
						COIL20.mat</a>. In the following parts, $ denotes the shell prompt
					while &gt;&gt;&gt; denotes the Python interpreter prompt:</font>
			</p>
			<font size="3" font="" style="line-height:2;">
				<pre>		     <p><font size="2" font="" style="line-height:0.5;">$ python</font></p>
		     <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;import scipy.io</font></p>
		     <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;mat = scipy.io.loadmat("COIL20.mat")</font></p>
		     </pre>
			</font>
		</div>
		<font size="3" font="" style="line-height:2;">
			<p>The loaded dataset is a dictionary-like object.
				Features of all instances are stored in mat['X'], and the ground truth of class labels are stored in
				mat['Y'].</p>
			<p>For instance, in the COIL20 dataset, mat['X'] is the matrix format of features:</p>
			<pre>		     <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;X=mat['X']</font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt;print X</p>
		     <p>&gt;&gt;&gt;[ 0.01568627  0.01568627  0.01568627 ...,   0.01568627  0.01568627  0.01568627]</p>
			 <p>&nbsp;&nbsp;&nbsp;[ 0.01960784  0.01960784  0.01960784 ...,   0.01960784  0.01960784  0.01960784]</p>
			 <p>&nbsp;&nbsp;&nbsp;[ 0.01568627  0.01568627  0.01568627 ...,   0.01568627  0.01568627  0.01568627]</p>
			 <p>&nbsp;&nbsp;&nbsp;...,</p>
			 <p>&nbsp;&nbsp;&nbsp;[ 0.          0.          0.         ...,   0.          0.          0.        ]</p>
			 <p>&nbsp;&nbsp;&nbsp;[ 0.          0.          0.         ...,   0.          0.          0.        ]</p>
			 </font><p><font size="2" font="" style="line-height:0.5;">&nbsp;&nbsp;&nbsp;[ 0.          0.          0.         ...,   0.          0.          0.        ]]</font></p>
		     </pre>
			<p>And mat['Y'] is the vector format of ground truth of class labels:</p>
			<pre><code>
		     <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;y = mat['Y'][:, 0] </font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt;print y</p>
		     </font><p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;[  1.   1.   1. ...,   20.   20.   20.]</font></p>
		     </code></pre>

			<h3 class="text-info">Shape of the data arrays</h3>
			<p>The feature matrix is always represented by a 2D array, in the shape of (n_samples, n_features). For
				example, in dataset <a href="http://featureselection.asu.edu/files/datasets/COIL20.mat">COIL20.mat</a>,
				the function numpy.shape outputs the shape of the features:</p>
			<pre><code>
			 <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;import numpy as np</font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt;n_samples, n_features = np.shape(X)</p>
		     <p>&gt;&gt;&gt;print n_samples, n_features</p>
		     </font><p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;1440 1024</font></p>
		     </code></pre>
			<p>The label is always represented by a 1D vector, in the shape of (n_labels,):</p>
			<pre><code>
		     <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;n_labels = np.shape(y)</font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt;print n_labels</p>
		     </font><p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;(1440L,)</font></p>
		     </code></pre>

			<hr class="featurette-divider">

			<h2 class="text-info"><b>For Supervised Learning Problems</b></h2>
			<h3 class="text-info">Split Data into Training and Testing Set</h3>
			<p>The function sklearn.cross_validation.train_test_split splits the data into train and test sets. Here we
				set the size of test data to be 20%:</p>
			<pre><code>
		     <p><font size="2" font="" style="line-height:0.5;">from sklearn.cross_validation import train_test_split</font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(</p>
             <p>...     X, y, test_size=0.2, random_state=40)</p>
		     <p>&gt;&gt;&gt;print X_train</p>
		     <p>array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ],</p>
             <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ 0.02352941,  0.02352941,  0.02352941, ...,  0.02352941,  0.02352941,  0.02352941],</p>
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ],</p>
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...,</p> 
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ 0.01568627,  0.01568627,  0.01568627, ...,  0.01568627,  0.01568627,  0.01568627],</p>  
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ],</p>
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ]])</p>
		     <p>&gt;&gt;&gt;print X_test</p>
		     <p>array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ],</p>
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ],</p>
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ 0.01568627,  0.01568627,  0.01568627, ...,  0.01568627,  0.01568627,  0.01568627],</p>
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;..., </p>
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ],</p>
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ],</p>
       		 <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ]])</p>
		     <p>&gt;&gt;&gt;print y_train</p>
			 <p>[17  2 12 ...,  1  4 19]</p>
		     <p>&gt;&gt;&gt;print y_test</p>
			 </font><p><font size="2" font="" style="line-height:0.5;">[8  15  1 ...,  15 17  7]</font></p>
		     </code></pre>

			<h3 class="text-info">Perform Feature Selection on the Training Set</h3>
			<p>We take Fisher Score algorithm as an example to explain how to perform feature selection on the training
				set. First, we compute the fisher scores of all features using the training set.
			</p>
			<p>Compute fisher score and output the score of each feature:</p>
			<pre><code>
			 <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;from skfeature.function.similarity_based import fisher_score</font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt;score = fisher_score.fisher_score(X_train, y_train)</p>
		     <p>&gt;&gt;&gt;print score</p>
		     </font><p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;[ 13.96904931   0.5376816    0.19923194 ...,   3.71944606  14.01720752  14.05075518]</font></p>
  			 </code></pre>
			<p>Rank features in an descending order according to fisher scores and outputs the ranking index:</p>
			<pre><code>
		     <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;idx = fisher_score.feature_ranking(score)</font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt;print idx</p>
		     </font><p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;[1023 1022   31 ...,   34   97  897]</font></p>
		     </code></pre>
			<p>Specify the number of selected features (e.g., 5) for the evaluation purpose:</p>
			<pre><code>
		     <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;num_fea = 5</font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt;selected_features_train = X_train[:, idx[0:num_fea]]</p>
		     <p>&gt;&gt;&gt;selected_features_test = X_test[:, idx[0:num_fea]]</p>
		     <p>&gt;&gt;&gt;print selected_features_train</p>
		     <p>&gt;&gt;&gt;[[ 0.          0.          0.          0.          0.        ]</p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.02352941  0.02352941  0.02352941  0.02352941  0.02352941]</p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.          0.          0.          0.          0.        ]</p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;..., </p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.01568627  0.01568627  0.01568627  0.01568627  0.01568627]</p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.          0.          0.          0.          0.        ]</p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.          0.          0.          0.          0.        ]]</p>
		     <p>&gt;&gt;&gt;print selected_features_test</p>
		     <p>&gt;&gt;&gt;[[ 0.          0.          0.          0.          0.        ]</p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.          0.          0.          0.          0.        ]</p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.01568627  0.01568627  0.01568627  0.01568627  0.01568627]</p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;..., </p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.          0.          0.          0.          0.        ]</p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.          0.          0.          0.          0.        ]</p>
			 </font><p><font size="2" font="" style="line-height:0.5;">&nbsp;&nbsp;&nbsp;&nbsp;[ 0.          0.          0.          0.          0.        ]]</font></p>
			 </code></pre>

			<h3 class="text-info">Training a Classification Model with Selected Features</h3>
			<p>Here we choose the linear SVM as an example:</p>
			<pre><code>
			 <p><font size="2" font="" style="line-height:0.5;">from sklearn import svm</font></p><font size="2" font="" style="line-height:0.5;">
			 </font><p><font size="2" font="" style="line-height:0.5;">clf = svm.LinearSVC()</font></p>
			 </code></pre>
			<p>Then we train a classification model with the selected features on the training set:</p>
			<pre><code>
			 <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;clf.fit(selected_features_train, y_train)</font></p>
			 </code></pre>

			<h3 class="text-info">Prediction Phase</h3>
			<p>Predict the class labels of test data based on the trained model</p>
			<pre><code>
			 <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;y_predict = clf.predict(selected_features_test)</font></p><font size="2" font="" style="line-height:0.5;">
			 <p>&gt;&gt;&gt;print y_predict</p>
			 </font><p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;[19 19  2 ..., 19 19 19]</font></p>
			 </code></pre>

			<h3 class="text-info">Performance Evaluation</h3>
			<p>Here, we use classification accuracy to measure the performance of supervised feature selection algorithm
				Fisher Score:</p>
			<pre><code>
			 <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;from sklearn.metrics import accuracy_score</font></p><font size="2" font="" style="line-height:0.5;">
			 <p>&gt;&gt;&gt;acc = accuracy_score(y_test, y_predict)</p>
			 <p>&gt;&gt;&gt;print acc</p>
			 </font><p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;0.09375</font></p>
			 </code></pre>

			<hr class="featurette-divider">

			<h2 class="text-info"><b>For Unsupervised Learning Problems</b></h2>

			<h3 class="text-info">Feature Selection</h3>
			<p>For unsupervised learning problems, we do not need to specify the training and testing set. In other
				words, we use the whole dataset for feature selection. Here, we use the Laplacian Score as an example
				to explain how to perform unsupervised feature selection. </p>
			<p>First, we construct affinity matrix which is required by Laplacian Score:</p>
			<pre><code>
			 <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;from skfeature.utility import construct_W</font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt;kwargs_W = {"metric":"euclidean","neighbor_mode":"knn","weight_mode":"heat_kernel","k":5,'t':1}</p>
		     </font><p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;W = construct_W.construct_W(X, **kwargs_W)</font></p>
  			 </code></pre>
			<p>Compute and output the score of each feature</p>
			<pre><code>
			 <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;from skfeature.function.similarity_based import lap_score</font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt;score = lap_score.lap_score(X, W=W)</p>
		     <p>&gt;&gt;&gt;print score</p>
		     </font><p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;[ 0.01269462  0.00637613  0.00333286 ...,  0.0123851   0.01271441   0.01269681]</font></p>
  			 </code></pre>
			<p>Rank features in an ascending order according to laplacian scores and output the ranking index:</p>
			<pre><code>
		     <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;idx = lap_score.feature_ranking(score)</font></p><font size="2" font="" style="line-height:0.5;">
		     </font><p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;[ 34  65 966 ...,  28 963 996]</font></p>
		     </code></pre>
			<p>Specify the number of selected features (e.g., 5) for the evaluation purpose:</p>
			<pre><code>
		     <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;num_fea = 5</font></p><font size="2" font="" style="line-height:0.5;">
		     <p>&gt;&gt;&gt;selected_features = X[:, idx[0:num_fea]]</p>
		     <p>&gt;&gt;&gt;[[ 0.01568627  0.01568627  0.01568627  0.01568627  0.01568627]</p>
		     <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.01960784  0.01960784  0.01960784  0.01960784  0.01960784]</p>
		     <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.01568627  0.01568627  0.01568627  0.01568627  0.01568627]</p>
	         <p>&nbsp;&nbsp;&nbsp;&nbsp;..., </p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.          0.          0.          0.          0.        ]</p>
			 <p>&nbsp;&nbsp;&nbsp;&nbsp;[ 0.          0.          0.          0.          0.        ]</p>
			 </font><p><font size="2" font="" style="line-height:0.5;">&nbsp;&nbsp;&nbsp;&nbsp;[ 0.          0.          0.          0.          0.        ]]</font></p>		     
			 </code></pre>

			<h3 class="text-info">Performance Evaluation</h3>
			<p>Here, we use normalized mutual infomation score (NMI) and accuracy (ACC) to measure the performance of
				unsupervised feature selection algorithm Laplacian Score. Usually, the parameter <i>n_clusters</i> is
				set to be the same as the number of classes in the ground truth.</p>
			<pre><code>
			 <p><font size="2" font="" style="line-height:0.5;">&gt;&gt;&gt;from skfeature.utility import unsupervised_evaluation</font></p><font size="2" font="" style="line-height:0.5;">
			 <p>&gt;&gt;&gt;import numpy as np</p><p>
			 </p><p>&gt;&gt;&gt;num_cluster = len(np.unique(y))</p><p>
			 </p><p>&gt;&gt;&gt;print num_cluster</p><p>
			 </p><p>&gt;&gt;&gt;20</p><p>
			 </p><p>&gt;&gt;&gt;nmi,acc=unsupervised_evaluation.evaluation(X_selected=selected_features,n_clusters=num_cluster,y=y)</p>
			 <p>&gt;&gt;&gt;print nmi</p><p>
			 </p><p>&gt;&gt;&gt;0.415270585545</p><p>
			 </p><p>&gt;&gt;&gt;print acc</p><p>
			 </p><p>&gt;&gt;&gt;0.197222222222</p></font></code><font size="2" font="" style="line-height:0.5;"><p><code>
			 </code></p></font></pre>
			<font size="2" font="" style="line-height:0.5;">

				<p class="pull-right"><a href="tutorial.html#">Back to top</a></p>
				<!-- Bootstrap core JavaScript
    ================================================== -->
				<!-- Placed at the end of the document so the pages load faster -->
				<script src="./Tutorial _ Feature Selection @ ASU_files/jquery.min.js.download"></script>
				<script src="./Tutorial _ Feature Selection @ ASU_files/bootstrap.min.js.download"></script>
				<!-- Just to make our placeholder images work. Don't actually copy the next line! -->
				<script src="./Tutorial _ Feature Selection @ ASU_files/holder.min.js.download"></script>
				<!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
				<script
					src="./Tutorial _ Feature Selection @ ASU_files/ie10-viewport-bug-workaround.js.download"></script>
				<script src="./Tutorial _ Feature Selection @ ASU_files/tooltip-viewport.js.download"></script>


			</font>
		</font>
	</div>
</body>

</html>