<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!-- saved from url=(0048)http://featureselection.asu.edu/old/research.php -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="keywords" content="">
<meta name="description" content="">

<title>Research | Feature Selection @ ASU</title>
<link href="./Research _ Feature Selection @ ASU_files/style.css" rel="stylesheet" type="text/css" media="screen">
<link rel="shortcut icon" href="http://featureselection.asu.edu/old/images/favicon.ico">

<script type="text/javascript" async="" src="./Research _ Feature Selection @ ASU_files/ga.js.download"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-18315668-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>
<body data-gr-c-s-loaded="true">

<div id="header">
	<div id="logo">
		<h1><a href="http://featureselection.asu.edu/old/research.php#">Feature Selection</a></h1>
		<p>at <a href="http://asu.edu/">Arizona State University</a></p>
		<br>
		In conjunction with the <a href="http://dmml.asu.edu/">DMML</a>.
	</div>
</div><!-- end #header -->
<div id="menu">
	<ul>
	<li><a href="http://featureselection.asu.edu/old/index.php">Home</a></li>
	<li><a href="http://featureselection.asu.edu/old/research.php">Research</a></li>
	<li><a href="http://featureselection.asu.edu/old/datasets.php">Datasets</a></li>
	<li><a href="http://featureselection.asu.edu/old/software.php">Feature Selection Algorithms</a></li>
	<li><a href="http://featureselection.asu.edu/old/donateds.php">Donate a Dataset</a></li>
	<li><a href="http://featureselection.asu.edu/old/donateal.php">Donate an Algorithm</a></li>
	<!--<li><a href="/forum/">Public Forum (beta)</a></li>-->
</ul>	
</div>
<!-- end #menu -->
<div id="page">
	<div id="content">
		<div class="post">
			<h1 class="title"><a href="http://featureselection.asu.edu/old/research.php#">Research</a></h1>
			
			<ul>
				<li><a href="http://featureselection.asu.edu/old/research.php#grants">Grants</a></li><a href="http://featureselection.asu.edu/old/research.php#grants">
				</a><li><a href="http://featureselection.asu.edu/old/research.php#grants"></a><a href="http://featureselection.asu.edu/old/research.php#research">Research</a></li><a href="http://featureselection.asu.edu/old/research.php#research">
			</a></ul><a href="http://featureselection.asu.edu/old/research.php#research">
			
			</a><div class="entry"><a href="http://featureselection.asu.edu/old/research.php#research">
				</a><a name="grants"><h2 class="STYLE1">NSF Grant</h2></a>
				<h4 class="STYLE1">Description</h4>
				<p>High-dimensional data is ubiquitous in real-world applications - from text categorization, to image processing, and to Web searches. The shortage of labeled data, resulting from high labeling costs, necessitates the need to explore machine learning approaches beyond classic classification and clustering paradigms. Semi-supervised learning is one such approach that demonstrates its potential in handling data with small labeled samples and reducing the need for expensive labeled data. However, high-dimensional data with small labeled samples permits too large a hypothesis space yet with too few constraints (labeled instances). The combination of the two data characteristics manifests a new research challenge. Employing computational and statistical learning theory, we analyze specific challenges presented by such data, show preliminary studies, delineate the need to integrate feature selection and extraction in a novel framework to reduce hypothesis space, propose to design efficient and novel algorithms, and conduct theoretical and empirical studies to understand complex relationships between high-dimensional data and classification performance.</p>
				<h4 class="STYLE1">Publications</h4>
				<ul style="margin-top:0in" type="disc"> 
				 <li class="MsoNormal"><b>Journal Articles</b> </li> 
				 <ul style="margin-top:0in" type="circle"> 
				  <li class="MsoNormal">Z. Zhao and H. Liu. "Multi-Source Feature Selection
				      via Geometry-Dependent Covariance Analysis", JMLR Workshop and
				      Conference Proceedings Volume 4: New challenges for feature selection in
				      data mining and knowledge discovery, 4:36-47, 2008</li> 
				  <li class="MsoNormal">Z. Zhao and H. Liu. "Searching for Interacting
				      Features in Subset Selection", Intelligent Data Analysis - An
				      International Journal, 13:207-228, 2009. </li> 
				  <li class="MsoNormal">M. Berens, H. Liu, L. Parsons, L. Yu, and Z. Zhao.
				      “Fostering Biological Relevance in Feature Selection for Microarray
				      Data”, Trends and Controversies,[<a href="http://www.public.asu.edu/~huanliu/papers/ieeeis05.pdf">PDF]</a>, pp
				      71 - 73. November/December 2005, IEEE Intelligent Systems.</li> 
				  <li class="MsoNormal">H. Liu and L. Yu. "Toward Integrating Feature
				      Selection Algorithms for Classification and Clustering", <a href="http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/trans/tk/&amp;toc=comp/trans/tk/2005/04/k4toc.xml&amp;DOI=10.1109/TKDE.2005.66">IEEE
				      Trans. on Knowledge and Data Engineering</a>,&nbsp;<a href="http://www.public.asu.edu/~huanliu/papers/tkde05.pdf">pdf</a>,
				      17(4), 491-502, 2005. </li> 
				  <li class="MsoNormal">Jieping Ye, Jianhui Chen, Ravi Janardan, and Sudhir
				      Kumar. Developmental Stage Annotation of <i>Drosophila</i> Gene Expression
				      Pattern Images via an Entire Solution Path for LDA. <b><i><a href="http://tkdd.cs.uiuc.edu/">ACM Transactions on Knowledge Discovery
				      from Data</a></i></b>. special issue on Bioinformatics. Vol. 2, No. 1,
				      pp. 1-21, 2008. [ <a href="http://portal.acm.org/citation.cfm?id=1342320.1342324&amp;coll=portal&amp;dl=ACM&amp;idx=J1054&amp;part=transaction&amp;WantType=Transactions&amp;title=TKDD&amp;CFID=49478026&amp;CFTOKEN=97837260">PDF</a>]</li> 
				 </ul> 
				</ul> 

				<ul style="margin-top:0in" type="disc"> 
				 <li class="MsoNormal"><b>Conferences and Workshops</b> 
				   <ul> 
						<li> Z. Zhao, L. Wang, and H. Liu. Efficient Spectral Feature Selection with Minimum Redundancy. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2010 . [<a href="http://www.public.asu.edu/~zzhao15/papers/zhengzhao_AAAI_2010.pdf">PDF</a>, <a href="http://www.public.asu.edu/~zzhao15/papers/zhengzhao_AAAI_2010_appendix.pdf">Supplementary</a>]</li> 
						<li> Z. Zhao, J. Wang, S. Sharma, N. Agarwal, H. Liu, and Y. Chang. An Integrative Approach to Identifying Biologically Relevant Genes. In Proceedings of SIAM International Conference on Data Mining (SDM), 2010. [<a href="http://www.public.asu.edu/~zzhao15/papers/SDM-ZZHAO-10.pdf">PDF</a>]</li> 

						<li> Z. Zhao, J. Wang, H. Liu, and Y. Chang. Biological relevance detection via network dynamic analysis. In Proceedings of 2nd International Conference on Bioinformatics and Computational Biology (BICoB), 2010. <a href="http://picasaweb.google.com/Mary.Ann.Sullivan2/CATABICoB2010PICS?feat=email#5455352633272872050">BEST PAPER AWARD</a> [<a href="http://www.public.asu.edu/~zzhao15/papers/BICoB-ZZHAO-10.pdf">PDF</a>]</li> 

				        <li>J. Liu, L. Yuan, and J. Ye. An Efficient Algorithm for a Class of Fused Lasso Problems. The Sixteenth ACM SIGKDD International Conference On Knowledge Discovery and Data Mining (SIGKDD 2010). [<a href="http://www.public.asu.edu/~jye02/Publications/Papers/rp589f-liu.pdf">PDF</a>] </li> 
				        <li>L. Sun, B. Ceran, and J. Ye. A Scalable Two-Stage Approach for a Class of Dimensionality Reduction Techniques. The Sixteenth ACM SIGKDD International Conference On Knowledge Discovery and Data Mining (SIGKDD 2010). </li> 
				        <li>J. Chen, J. Liu, and J. Ye. Learning Incoherent Sparse and Low-Rank Patterns from Multiple Tasks. The Sixteenth ACM SIGKDD International Conference On Knowledge Discovery and Data Mining (SIGKDD 2010). <br> 
				        </li> 
				        <li>H. Liu, H. Motoda, R. Setiono, and Z. Zhao. Feature Selection: An Ever Evolving Frontier in Data Mining, Journal of Machine Learning Research, Workshop and Conference Proceedings Volume 10, 10:4-13, 2010.[<a href="http://featureselection.asu.edu/old/papers/FSDM2010-liu10b.pdf">PDF</a>]</li> 
				        <li>L. Sun, J. Liu, J. Chen, and J. Ye. Efficient Recovery of Jointly Sparse Vectors. The Twenty-Third Annual Conference on Neural Information Processing Systems (NIPS 2009). [<a href="http://www.public.asu.edu/~jye02/Publications/Papers/NIPS09_1056.pdf">PDF</a>] </li> 
				        <li>J. Liu, S. Ji, and J. Ye. Multi-task Feature Learning via Efficient L2,1-Norm Minimization. The Twenty-fifth Conference on Uncertainty in Artificial Intelligence (UAI 2009).[<a href="http://www.public.asu.edu/~jye02/Publications/Papers/Liu_UAI09.pdf">PDF</a>] </li> 
				        <li>J. Liu, J. Chen, and J. Ye. Large-Scale Sparse Logistic Regression. The Fifteenth ACM SIGKDD International Conference On Knowledge Discovery and Data Mining (SIGKDD 2009), pp. 547-556. </li> 
				        <li>L. Sun, S. Ji, and J. Ye. A Least Squares Formulation for a Class of Generalized Eigenvalue Problems in Machine Learning. The Twenty-Sixth International Conference on Machine Learning (ICML 2009). [<a href="http://www.cs.mcgill.ca/~icml2009/papers/315.pdf">PDF</a>] </li> 
				        <li>S. Ji and J. Ye. Linear Dimensionality Reduction for Multi-label Classification. The Twenty-first International Joint Conference on Artificial Intelligence (IJCAI 2009).[<a href="http://www.public.asu.edu/~jye02/Publications/Papers/mldr-ijcai09.pdf">PDF</a>] </li> 
				        <li>Z. Zhao, J. Wang, S. Sharma, N. Agarwal, H. Liu and Y. Chang. " A Knowledge-Oriented Framework for Gene Selection", Poster. Tuscon, Arizona, May 18-21. RECOMB'09</li> 
				     <li>Z. Zhao, L. Sun, S. Yu, H. Liu, J. Ye. "Multiclass Probabilistic Kernel Discriminant Analysis", IJCAI'09 [<a href="http://www.public.asu.edu/~zzhao15/papers/ZZHAO-IJCAI-1453.pdf">PDF</a>] </li> 
				   </ul> 
				 </li> 
				 <ul style="margin-top:0in" type="circle"> 
				  <li class="MsoNormal">Z. Zhao, J. Wang, H. Liu, J. Ye, and Y. Chang.
				      "Identifying Biologically Relevant Genes via Multiple Heterogeneous
				      Data Sources", KDD'08: 839 - 847. [<a href="http://www.public.asu.edu/~huanliu/papers/kdd08Zhao.pdf">PDF</a>]</li> 
				  <li class="MsoNormal">Z. Zhao and H. Liu. ``Spectral Feature Selection for
				      Supervised and Unsupervised Learning''. International Conference on Machine
				      Learning (ICML-07), June 20-24, 2007, Corvallis, Oregon. [<a href="http://www.public.asu.edu/~huanliu/papers/icml07.pdf">PDF</a>]</li> 
				  <li class="MsoNormal">Z. Zhao and H. Liu. ``Semi-supervised Feature Selection
				      via Spectral Analysis", SIAM International Conference on Data Mining
				      (<a href="http://www.siam.org/meetings/sdm07/">SDM-07</a>), April&nbsp;
				      26-28, 2007, Minneapolis, Minnesoda. [<a href="http://www.public.asu.edu/~huanliu/papers/sdm07.pdf">PDF</a>]</li> 
				  <li class="MsoNormal">Z. Zhao and H. Liu. ``Searching for Interacting
				      Features", The 20th International Joint Conference on AI (<a href="http://www.ijcai-07.org/">IJCAI-07</a>), January 6-12 Hyderabad,
				      India. [<a href="http://www.public.asu.edu/~huanliu/papers/ijcai07.pdf">PDF</a>].
				      <a href="http://www.public.asu.edu/~huanliu/INTERACT/INTERACTsoftware.html">Software
				      available</a>. </li> 
				  <li class="MsoNormal">Jieping Ye<b>. </b>Least Squares Linear Discriminant
				      Analysis.<b> </b><i>The Twenty-Fourth International Conference on Machine
				      Learning</i> (<a href="http://oregonstate.edu/conferences/icml2007/">ICML
				      2007</a>), pp. 1087-1093<i>.</i> Technical Report <span style="color:blue"><a href="http://sci.asu.edu/news/technical/TR_year.php?pub_date=2006">TR-06-003</a></span>,
				      Department of Computer Science and Engineering, Arizona State University
				      , March, 2006. [<a href="http://www.public.asu.edu/~jye02/Publications/PDF/LS-LDA_icml07.pdf">PDF</a>]</li> 
				 </ul> 
				</ul> 

				<ul style="margin-top:0in" type="disc"> 
				 <li class="MsoNormal"><b>Books or Chapters</b> </li> 
				 <ul style="margin-top:0in" type="circle"> 
				  <li class="MsoNormal">Huan Liu and Hiroshi Motoda, "Feature Selection for
				      Knowledge Discovery and Data Mining", July 1998, ISBN 0-7923-8198-X,
				      by <a href="http://www.wkap.nl/book.htm/0-7923-8198-X">Kluwer Academic
				      Publishers</a> </li> 
				  <li class="MsoNormal">Huan Liu and Hiroshi Motoda, “Computational Methods of
				      Feature Selection”, editors, 2008, Chapman and Hall/CRC Press. </li> 
				  <li class="MsoNormal">H. Liu and Z. Zhao. "Manipulating Data and
				      Dimensionality Reduc-tion Methods: Feature Selection", in
				      Encyclopedia of Complexity and Systems Science, Robert Meyers (Ed.),
				      Springer.&nbsp; 2009. </li> 
				  <li class="MsoNormal">H. Liu. "Feature Selection: An Overview", in
				      Encyclopedia of Machine Learning, Claude Sammut (Ed.), Springer.
				      Forthcoming.</li> 
				  <li class="MsoNormal">Z. Zhao and H. Liu. "On Interacting Features in
				      Subset Selection", in Encyclopedia of Data Warehousing and Mining,
				      2nd Edition, Idea Group, Inc. pp 1079 -- 1084, September, 2008.</li> 
				 </ul> 
				</ul> 

				<ul style="margin-top:0in" type="disc"> 
				 <li class="MsoNormal"><b>Technical Reports</b> </li> 
				 <ul style="margin-top:0in" type="circle"> 
				  <li class="MsoNormal">Z. Zhao and H. Liu. ``Semi-supervised Feature Selection
				      via Spectral Analysis", Technical Report, <a href="http://www.public.asu.edu/~huanliu/papers/ssfs.pdf">TR-06-022</a>,
				      Department of Computer Science and Engineering, Arizona State University,
				      Tempe, AZ 85287, 2006. </li> 
				  <li class="MsoNormal">Y. Ye, L. Yu, and H. Liu.&nbsp; ``Sparse Linear
				      Discriminant Analysis", Technical Report, TR-06-010, Department of
				      Computer Science and Engineering, Arizona State University, Tempe, AZ
				      85287, 2006.</li> 
				  </ul> 
				  <li><strong>Thesis</strong></li> 
				  <ul> 
				  <li> Z. Zhao. Spectral Feature Selection for Mining Ultrahigh Dimensional Data [<a href="http://www.public.asu.edu/~zzhao15/papers/zhaozheng_2010_thesis.pdf">PDF</a>] </li> 
				  </ul>  
				  <li><strong>Resources </strong></li> 
				  <ul> 
				  <li> <a href="http://featureselection.asu.edu/">Feature Selection Repository</a></li> 
				  <li> <a href="http://www.public.asu.edu/~jye02/Software/SLEP/index.htm">SLEP: A Sparse Learning Package</a></li> 
				  </ul> 
				  </ul> 



				<h2>Related Activities</h2> 

				<ul style="margin-top:0in" type="disc"> 
				 <li class="MsoNormal"><strong>Workshop on Feature Selection in Data Mining (FSDM 10)</strong> [<a href="http://featureselection.asu.edu/">link</a>]<br> 
				The proceedings of FSDM 2010 has been published by <a href="http://jmlr.csail.mit.edu/proceedings/papers/v10/">JMLR Workshop and Conference Proceedings</a></li> 
				 <li class="MsoNormal"> <strong>Tutorial  at SDM10:&nbsp;</strong><a href="http://www.public.asu.edu/~jye02/SDM10/Sparse-SDM10.pdf">Mining Sparse Representations: Formulations, Algorithms, and Applications</a> </li> 
				 <li class="MsoNormal"><b>SIAM Data Mining SDM 2007 Tutorial: </b><span style="color:blue"><a href="http://www.cs.binghamton.edu/~lyu/SDM07/DR-SDM07.pdf">Dimensionality
				   Reduction for Data Mining - Techniques, Applications, and Trends</a></span> </li> 
				 <li class="MsoNormal"><b>AAAI
				     2005 Tutorial: </b><a href="http://www.aaai.org/Conferences/National/2005/tutorials05.html">Notes on Downsizing Data for High Performance in
				     Learning - Feature Selection Methods</a>, <a href="http://www.public.asu.edu/~huanliu/papers/AAAI05-FeatureSelectionTutorial.zip">pdf.zip</a>.    </li> 
				</ul> 

				<h2>Project Members</h2> 

				<ul style="margin-top:0in" type="disc"> 
				 <li class="MsoNormal"><a href="http://www.public.asu.edu/~huanliu/">Huan Liu</a> 
				     (PI)</li> 
				 <li class="MsoNormal"><a href="http://www.public.asu.edu/~jye02/">Jieping Ye</a> 
				     (Co-PI)</li> 
				 <li class="MsoNormal"><a href="http://www.public.asu.edu/~zzhao15/">Zheng Zhao</a> (Successfully defensed his PhD dissertation, joined SAS institute)</li> 
				 <li class="MsoNormal"> <a href="mailto:salelyan@asu.edu">Salem Alelyani</a> (Graduate, PhD)</li> 
				 <li class="MsoNormal"><a href="http://www.public.asu.edu/~lyuan9/">Lei Yuan</a> (Graduate, PhD) </li> 
				 <li class="MsoNormal"> <a href="mailto:sssharma@asu.edu">Shashvata Sharma</a> (Finished her graduate study for master degree, joined Microsoft)</li> 
				 <li class="MsoNormal"><a href="mailto:Fred.Morstatter@asu.edu">Fred Morstatter</a> (Undergraduate) </li> 
				 <li class="MsoNormal"> <a href="mailto:aanand11@asu.edu">Aneeth Anand</a> (Graduate, Master) </li> 
				</ul> 

				<h2>Acknowledgments</h2> 

				<p class="MsoNormal">This project is sponsored by NSF (#0812551), 9/2008 -
				8/2011. </p>
				
			</div>
			
			<div class="entry">
				<a name="research"><h2 class="STYLE1">Description of Research</h2></a>
			  <p>Feature selection aims to choose a subset of original  features according to a selection criterion. It is an important technique that  is widely used in pattern analysis. Feature selection removes irrelevant and  redundant features and brings about many benefits: giving more reliable  parameter estimates, reducing computational cost and memory usage, improving  learning performance, and providing better result comprehensibility [Guyo-Elis03,Liu-Moto98c].  According to the way of utilizing label information, feature selection  algorithms can be categorized as supervised algorithms [West-etal03, Robn-Kono03],  unsupervised algorithms [Dy-Brod04,He-etal05] or semi-supervised algorithms [zhao-sdm07,xu-ijcai-09].  From the perspective of selection strategy, feature selection algorithms  broadly fall into three models: filter, wrapper or embedded [Guyo-Elis03]. The filter  model evaluates features without involving any learning algorithm. The wrapper  model requires a learning algorithm and uses its performance to evaluate the  goodness of features. Algorithms of the embedded model, e.g., C4.5 [Quin93] and  LARS [efro-etal04], incorporate feature selection as a part of learning  process, and use the objective function of the learning model to guide  searching for relevant features. In addition, feature selection algorithms may  return either a subset of features [Yu-Liu03,Hall00] or the weights of all  features measuring their utility [Aha98,Robn-Kono03]. Hence, they can also be  categorized as subset selection algorithms or feature weighting algorithms. As  an important technique, feature selection has been applied to various areas:  computer vision [Dy-etal03], text mining [Geor03], and bioinformatics [Saeys2007],  to name a few.</p>
			  <p>The task of this depository is to collect the most popular  algorithms that have been developed in the feature selection  research area to serve as a platform to facilitate their application, comparison and  joint study. You are encouraged to donate your algorithms and data sets to our depository.</p>
			  <p>-------------------Reference---------------------</p>
			  <p> 	<a name="Guyo-Elis03" id="Guyo-Elis03">[Guyo-Elis03</a>] Guyon, I. &amp; Elisseeff, A. An introduction  to variable and feature selection Journal of Machine Learning Research, 2003, 3,  1157-1182 <br>
				<a name="Liu-Moto98c" id="Liu-Moto98c">[Liu-Moto98c</a>] &nbsp;Liu, H. &amp; Motoda, H. Feature Selection for  Knowledge Discovery and Data Mining Boston: Kluwer Academic Publishers, 1998 <br>
				<a name="West-etal03" id="West-etal03">[West-etal03</a>] Weston, J.; Elisseff,  A.; Schoelkopf, B. &amp; Tipping, M. Use of the zero norm with linear models  and kernel methods Journal of Machine Learning Research, 2003, 3, 1439-1461 <br>
				<a name="Robn-Kono03" id="Robn-Kono03">[Robn-Kono03</a>] Sikonja, M. R. &amp;  Kononenko, I. Theoretical and empirical analysis of Relief and ReliefF Machine  Learning, 2003, 53, 23-69 <br>
				<a name="Dy-Brod04" id="Dy-Brod04">[Dy-Brod04</a>] Dy, J. G. &amp; Brodley,  C. E. Feature Selection for Unsupervised Learning J. Mach. Learn. Res., MIT  Press, 2004, 5, 845-889 <br>
				<a name="He-etal05" id="He-etal05">[He-etal05</a>] He, X.; Cai, D. &amp;  Niyogi, P. Weiss, Y.; Schölkopf, B. &amp; Platt, J. (ed.) Laplacian Score for  Feature Selection Advances in Neural Information Processing Systems 18, MIT  Press, 2005 <br>
				<a name="zhao-sdm07" id="zhao-sdm07">[zhao-sdm07</a>] Zhao, Z. &amp; Liu, H. Semi-supervised  Feature Selection via Spectral Analysis Proceedings of SIAM International  Conference on Data Mining (SDM), 2007 <br>
				<a name="xu-ijcai-09" id="xu-ijcai-09">[xu-ijcai-09</a>] Xu, Z.; Jin, R.; Ye,  J.; Lyu, M. R. &amp; King, I. Discriminative semi-supervised feature selection  via manifold regularization IJCAI' 09: Proceedings of the 21th International  Joint Conference on Artificial Intelligence, 2009 <br>
				<a name="Quin93" id="Quin93">[Quin93</a>] Quinlan, J. R. C4.5: Programs  for Machine Learning Morgan Kaufmann, 1993 <br>
				<a name="efro-etal04" id="efro-etal04">[efro-etal04</a>] Efron, B.; Hastie, T.;  Johnstone, I. &amp; Tibshirani, R. Least Angle Regression Annals of Statistics,  2004, 32, 407-49 <br>
				<a name="Yu-Liu03" id="Yu-Liu03">[Yu-Liu03</a>] Yu, L. &amp; Liu, H. Fawcett,  T. &amp; Mishra, N. (ed.) Feature Selection for High-Dimensional Data: A Fast  Correlation-Based Filter Solution Proceedings of the 20th International  Conference on Machine Learning (ICML-03),, Morgan Kaufmann, 2003, 856-863 <br>
				<a name="Hall00" id="Hall00">[Hall00</a>] Hall, M. A. Correlation-based  Feature Selection for Discrete and Numeric Class Machine Learning Proceedings  of the Seventeenth International Conference on Machine Learning, 2000, 359-366 <br>
				<a name="Aha98" id="Aha98">[Aha98</a>] Aha, D. W. Feature Weighting for  Lazy Learning Algorithms Feature Extraction, Construction and Selection: A Data  Mining Perspective, 1998, 13-32 <br>
				<a name="Dy-etal03" id="Dy-etal03">[Dy-etal03</a>] Dy, J. G.; Brodley, C. E.;  Kak, A. C.; Broderick, L. S. &amp; Aisen, M. A. Unsupervised Feature Selection  Applied to Content-Based Retrieval of Lung Images IEEE Transactions on Pattern  Analysis and Machine Intelligence, 2003, 25, 373-378 <br>
				<a name="Geor03" id="Geor03">[Geor03</a>] Forman, G. An Extensive  Empirical Study of Feature Selection Metrics for Text Classification Journal of  Machine Learning Research, 2003, 3, 1289-1305 <br>
				<a name="Saeys2007" id="Saeys2007">[Saeys2007</a>] Saeys, Y.; Inza, I. &amp;  Larrañaga, P. A review of feature selection techniques in bioinformatics. Bioinformatics,  2007, 23, 2507-2517
			  </p>
			</div>
		</div>
	</div>
	<!-- end #sidebar -->
	<div style="clear: both;">&nbsp;</div>
</div>
<!-- end #page -->
<div id="footer">
	<p>© 2020 DMML @ ASU. All rights reserved. <a href="http://featureselection.asu.edu/old/license.php">License</a>
</p></div>
<!-- end #footer -->


</body></html>